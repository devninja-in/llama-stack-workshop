from llama_stack_client import *
import os
from dotenv import load_dotenv
from llama_stack_client.types.shared_params import UserMessage, SystemMessage

load_dotenv()

# create llama stack client


def chat_completion_with_inference(content: str):
    #chat completion
    pass


if __name__ == "__main__":
    chat_completion_with_inference("tell me a story")